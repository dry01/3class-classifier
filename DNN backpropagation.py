# -*- coding: utf-8 -*-
"""backpropagation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gZ4RQJo_GTuX0k3HaigXCy_Mt22UFRfn

Dharamvir yadav(MT19CPS012)
3class backpropogation from sractch
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns # visualization
import matplotlib.pyplot as plt

# %matplotlib inline



from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/Jovian/iris_csv.csv')
data.sample(n=5)

data.describe()

df_norm = data[['sepallength', 'sepalwidth', 'petallength', 'petalwidth']].apply(lambda x: (x - x.min()) / (x.max() - x.min()))
df_norm.sample(n=5)

df_norm.describe()

target = data[['class']].replace(['Iris-setosa','Iris-versicolor','Iris-virginica'],[0,1,2])
target.sample(n=5)

df = pd.concat([df_norm, target], axis=1)
df.sample(n=5)

train_test_per = 90/100.0
df['train'] = np.random.rand(len(df)) < train_test_per
df.sample(n=5)

train = df[df.train == 1]
train = train.drop('train', axis=1).sample(frac=1)
train.sample(n=5)

test = df[df.train == 0]
test = test.drop('train', axis=1)
test.sample(n=5)

X = train.values[:,:4]
X.T.shape

targets = [[1,0,0],[0,1,0],[0,0,1]]
y = np.array([targets[int(x)] for x in train.values[:,4:5]])
y[:5]

X_a = test.values[:,:4]
print(X[:5])
targets = [[1,0,0],[0,1,0],[0,0,1]]
y_a = np.array([targets[int(x)] for x in test.values[:,4:5]])
print(y[:5])

len(X_a)

#initialise weight between input layer and hidden layer
num_inputs = len(X[0])
hidden_layer_neurons = 5
np.random.seed(4)
w1 = 2*np.random.random((num_inputs, hidden_layer_neurons)) - 1
w1

#initialise weight between hidden layer and output layer
num_outputs = len(y[0])
w2 = 2*np.random.random((hidden_layer_neurons, num_outputs)) - 1
w2

list = [0.5,0.05,0.005,0.005,0.005,0.0005,0.00005,0.0000005]
list1 = [5000]

for i in list1:
  
  learning_rate = 0.05 # slowly update the network
  error = []
  for epoch in range(i):
  
    # activate the first layer and second layer using the input
  
    #   result is fed into a sigmoid function
      l1 = 1/(1 + np.exp(-(np.dot(X, w1))))
    
      l2 = 1/(1 + np.exp(-(np.dot(l1, w2))))
    # find the average errorof this batch
    
      er = (abs(y - l2)).mean()
      error.append(er)
    
    # BACKPROPAGATION / learning!
    
      l2_delta = (y - l2)*(l2 * (1-l2))
      
    # update each weight in the second layer slowly
      w2 += l1.T.dot(l2_delta) * learning_rate
    
    # error on each weight on the second layer w.r.t the first layer
      l1_delta = l2_delta.dot(w2.T) * (l1 * (1-l1))
      
    # udpate weights in the first layer
      w1 += X.T.dot(l1_delta) * learning_rate
  print('Error:', er)

plt.plot(error)

learning_rate = 0.05

 
error1 = []
for epoch in range(5000):
    
    l1 = 1/(1 + np.exp(-(np.dot(X_a, w1))))
    
    l2 = 1/(1 + np.exp(-(np.dot(l1, w2))))
    
    
    er = (abs(y_a - l2)).mean()
    error1.append(er)
    
    
    l2_delta = (y_a - l2)*(l2 * (1-l2))
    
    
    w2 += l1.T.dot(l2_delta) * learning_rate
    
    
    l1_delta = l2_delta.dot(w2.T) * (l1 * (1-l1))
    
  
    w1 += X_a.T.dot(l1_delta) * learning_rate
print('Error:', er)

#plot training and test loss
plt.plot(error)
plt.plot(error1)

#NOW WE APPLLY L2 NORMALISATION

def mse(t1,t2):
  diff = t1- t2
  
  return np.sum(diff*diff)/ len(diff)

for i in list1:
  
  learning_rate = 0.05 # slowly update the network
  error = []
  for epoch in range(i):
  
    # activate the first layer and second layer using the input
  
    #   result is fed into a sigmoid function
      l1 = 1/(1 + np.exp(-(np.dot(X, w1))))
    
      l2 = 1/(1 + np.exp(-(np.dot(l1, w2))))
    # find the average errorof this batch
      er = mse (y,l2)
      #er = (abs(y - l2)).mean()
      error.append(er)
    
    # BACKPROPAGATION / learning!
    
      l2_delta = (y - l2)*(l2 * (1-l2))
      
    # update each weight in the second layer slowly
      w2 += l1.T.dot(l2_delta) * learning_rate
    
    # error on each weight on the second layer w.r.t the first layer
      l1_delta = l2_delta.dot(w2.T) * (l1 * (1-l1))
      
    # udpate weights in the first layer
      w1 += X.T.dot(l1_delta) * learning_rate
  print('Error:', er)

plt.plot(error)



learning_rate = 0.05

 
error1 = []
for epoch in range(5000):
    
    l1 = 1/(1 + np.exp(-(np.dot(X_a, w1))))
    
    l2 = 1/(1 + np.exp(-(np.dot(l1, w2))))
    
    er = mse (y_a,l2)
    
    
    error1.append(er)
    
    
    l2_delta = (y_a - l2)*(l2 * (1-l2))
    
    
    w2 += l1.T.dot(l2_delta) * learning_rate
    
    
    l1_delta = l2_delta.dot(w2.T) * (l1 * (1-l1))
    
  
    w1 += X_a.T.dot(l1_delta) * learning_rate
print('Error:', er)

plt.plot(error)
plt.plot(error1)

